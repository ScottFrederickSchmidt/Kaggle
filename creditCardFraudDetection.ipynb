{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Card Fraud Dectection\n\nThis will use machine learning and predictive modeling to help detect fradulent credit card transaction. Data is from transactions made by credit cards in September 2013 by european cardholders which can be found here: https://www.kaggle.com/mlg-ulb/creditcardfraud. \n\nThe columns with the first five rows can be seen below in the next section.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\n\nkaggleFile=r'/kaggle/input/creditcardfraud/creditcard.csv'\ndata=pd.read_csv(kaggleFile)\nprint(data.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:15:15.439727Z","iopub.execute_input":"2022-02-11T21:15:15.440353Z","iopub.status.idle":"2022-02-11T21:15:21.029459Z","shell.execute_reply.started":"2022-02-11T21:15:15.440293Z","shell.execute_reply":"2022-02-11T21:15:21.028306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find missing data","metadata":{}},{"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf = data.select_dtypes(include=numerics)\n\nprint(\"Missing data by column:\")\nfindNA=df.isnull().sum().sort_values(ascending=False)/len(data)\nprint(findNA.head()) #There are no missing values","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:15:21.031928Z","iopub.execute_input":"2022-02-11T21:15:21.032218Z","iopub.status.idle":"2022-02-11T21:15:21.114713Z","shell.execute_reply.started":"2022-02-11T21:15:21.032186Z","shell.execute_reply":"2022-02-11T21:15:21.113639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Unbalanced Dataset\nSince the amount of fraudelent transactions is 0.0017, the dataset is considered highly unbalanced.This means metrics used should be the following: recall, precision, and AOC. ","metadata":{}},{"cell_type":"code","source":"temp = df[\"Class\"].value_counts()\nfraud = temp[1]\nnotFraud= temp[0]\nfraudNumbers=round((fraud/notFraud),4)\nprint(fraudNumbers, \"percent of transactions are fraudelent.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:15:21.11639Z","iopub.execute_input":"2022-02-11T21:15:21.116685Z","iopub.status.idle":"2022-02-11T21:15:21.131429Z","shell.execute_reply.started":"2022-02-11T21:15:21.116652Z","shell.execute_reply":"2022-02-11T21:15:21.130408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlations using heatmap\nNo multicollinearity was detected. The highest correlation between features was V7 and Amount which had a 0.39730 correlation.","metadata":{}},{"cell_type":"code","source":"def printHeat():\n    corr = df.corr()\n    #print(corr)\n    highly_corr_features = corr.index[abs(corr[\"Class\"])>0.2]\n    plt.figure(figsize=(10,10))\n    heat = sns.heatmap(data[highly_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n    top10=corr[\"Class\"].sort_values(ascending=False).head(10).round(4)\n    print(heat)\n    #print(top10) # top ten correlations\n\n#print correlation between features\ndef printHighCorr(df, features, threshold=0.2):\n    print(\"Highly correlated variables above: \", threshold)\n    corr_df = df[features].corr() # get correlations\n    correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n    correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n    s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n    correlation_df={}\n    if s_corr_list == []:\n        print(\"There are no highly correlated features with correlation above\", threshold)\n    else:\n        for v, i, j in s_corr_list:\n            correlation_df[corr_df.index[i] +\" and \"+ corr_df.columns[j]]= v\n        correlation_df=pd.DataFrame(correlation_df,index=['Correlation']).round(4)\n    return  correlation_df.T.sort_values(by='Correlation',ascending=False)\n\n#Turn functions on or off below:\nprintHeat() \nprintHighCorr(data,data.columns).style.set_properties(**{'background-color': 'black','color': 'white'})","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:15:21.132748Z","iopub.execute_input":"2022-02-11T21:15:21.133033Z","iopub.status.idle":"2022-02-11T21:15:23.38679Z","shell.execute_reply.started":"2022-02-11T21:15:21.133Z","shell.execute_reply":"2022-02-11T21:15:23.385556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Data\nUse 70% of the train data to predict the accuracy of the remaining 30% of the test data.","metadata":{}},{"cell_type":"code","source":"X=df.drop('Class', axis=1)\ny=df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:15:23.391568Z","iopub.execute_input":"2022-02-11T21:15:23.39196Z","iopub.status.idle":"2022-02-11T21:15:23.558664Z","shell.execute_reply.started":"2022-02-11T21:15:23.391911Z","shell.execute_reply":"2022-02-11T21:15:23.557754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" #### Gradient Boost with Feature Importance","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:03:12.342338Z","iopub.execute_input":"2022-02-11T21:03:12.343184Z","iopub.status.idle":"2022-02-11T21:03:12.347636Z","shell.execute_reply.started":"2022-02-11T21:03:12.343138Z","shell.execute_reply":"2022-02-11T21:03:12.34641Z"}}},{"cell_type":"code","source":"#GRADIENT BOOST REGRESSION:\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\nfrom sklearn import ensemble\n\nparams = {\n \"n_estimators\": 500, \"max_depth\": 4, \"min_samples_split\": 5, \"learning_rate\": 0.01,\n}\n\nreg = ensemble.GradientBoostingRegressor(**params)\nreg.fit(X_train, y_train)\n\ny_pred = reg.predict(X_test)\ngbr_r2 = r2_score(y_test, y_pred).round(4) \nprint(\"Gradient boosting regression r2: \", gbr_r2) \n\nmse = mean_squared_error(y_test, reg.predict(X_test))\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n\n#FEATURE IMPORTANCE:\ncols=X.columns\nfeature_importance = reg.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + 0.5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align=\"center\")\nplt.yticks(pos, np.array(cols)[sorted_idx])\nplt.title(\"Feature Importance (MDI)\")\n\nresult = permutation_importance(\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\nplt.subplot(1, 2, 2)\nplt.boxplot(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=np.array(cols)[sorted_idx],\n)\nplt.title(\"Permutation Importance (test set)\")\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:15:23.560062Z","iopub.execute_input":"2022-02-11T21:15:23.560997Z","iopub.status.idle":"2022-02-11T21:53:13.210915Z","shell.execute_reply.started":"2022-02-11T21:15:23.560948Z","shell.execute_reply":"2022-02-11T21:53:13.209765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogReg = LogisticRegression()\n\n#Train the model and create predictions\nlogReg.fit(X_train, y_train)\npredictions = logReg.predict(X_test)\n\n#use model to predict probability that given y value is 1:\ny_pred_proba = logReg.predict_proba(X_test)[::,1]\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, y_pred_proba), 4 ) \nprint(\"AUC for logistic regression is: \", auc)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:53:13.211942Z","iopub.status.idle":"2022-02-11T21:53:13.212686Z","shell.execute_reply.started":"2022-02-11T21:53:13.212461Z","shell.execute_reply":"2022-02-11T21:53:13.212491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-Nearest Neigbors","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nprint(\"starting knn\")\n\n#Selecting an optimal K value:\nerror_rates = []\nfor i in range(1, 10, 2): #Must be an odd number to break a tie\n    new_model = KNeighborsClassifier(n_neighbors = i)\n    new_model.fit(X_train, y_train)\n    new_predictions = new_model.predict(X_test)\n    error_rates.append(np.mean(new_predictions != y_test))\nprint(\"finished error calculation\")\n\nplt.figure(figsize=(16,12))\nplt.plot(error_rates)\n#Error had the least amount at 1. \n\n#Train the model and make predictions:\nknn = KNeighborsClassifier(n_neighbors =1) \nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\n#Performance measurement:\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n#print(classification_report(y_test_data, predictions))\n#print(confusion_matrix(y_test_data, predictions))\n\n#use model to predict probability that given y value is 1:\ny_pred_proba = knn.predict_proba(X_test)[::,1]\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, y_pred_proba), 4 ) \nprint(auc)\n\n#acc = metrics.accuracy_score(y_test_data, predictions) #not needed\nprint(confusion_matrix(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:53:13.214185Z","iopub.status.idle":"2022-02-11T21:53:13.214611Z","shell.execute_reply.started":"2022-02-11T21:53:13.214424Z","shell.execute_reply":"2022-02-11T21:53:13.214448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Resources\n1. https://www.kaggle.com/jdelamorena/recall-97-by-using-undersampling-neural-network\n2. https://www.kaggle.com/gpreda/credit-card-fraud-detection-predictive-models","metadata":{}}]}