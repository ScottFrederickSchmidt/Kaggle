{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Detect Credit Card Fraud\nMachine learning will be used to detect fradulent credit card transaction. Data is from transactions made by credit cards in September 2013 by european cardholders which can be found here: https://www.kaggle.com/mlg-ulb/creditcardfraud. \n\nThe final predictive models using AUC is:\n* KNN 0.9844\n* Linear Regression 0.9844\n* Gradient boosting regression 0.93\n* Logistic regression 0.905\n\nThe train dataset has 5 rows Ã— 31 columns with the first five rows can be seen below in the next section.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\nfrom sklearn import metrics\n\nkaggleFile=r'/kaggle/input/creditcardfraud/creditcard.csv'\ndata=pd.read_csv(kaggleFile)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:05:41.175706Z","iopub.execute_input":"2022-02-20T18:05:41.176218Z","iopub.status.idle":"2022-02-20T18:05:44.528347Z","shell.execute_reply.started":"2022-02-20T18:05:41.176181Z","shell.execute_reply":"2022-02-20T18:05:44.527247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find missing data\nThere is no missing data.","metadata":{}},{"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf = data.select_dtypes(include=numerics)\n\nprint(\"Missing data by column:\")\nfindNA=df.isnull().sum().sort_values(ascending=False)/len(data)\nprint(findNA.head()) #There are no missing values","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:05:44.530527Z","iopub.execute_input":"2022-02-20T18:05:44.530833Z","iopub.status.idle":"2022-02-20T18:05:44.600903Z","shell.execute_reply.started":"2022-02-20T18:05:44.530799Z","shell.execute_reply":"2022-02-20T18:05:44.599888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unbalanced Dataset\nSince the amount of fraudelent transactions is 0.0017, the dataset is considered highly unbalanced. This means metrics used should be the following: recall, precision, and AOC. R-squared should not be used for an unbalanced dataset.","metadata":{}},{"cell_type":"code","source":"temp = df[\"Class\"].value_counts()\nfraud = temp[1]\nnotFraud= temp[0]\nfraudNumbers=round((fraud/notFraud),4)\nprint(fraudNumbers, \"percent of transactions are fraudelent.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:05:44.602239Z","iopub.execute_input":"2022-02-20T18:05:44.60261Z","iopub.status.idle":"2022-02-20T18:05:44.619039Z","shell.execute_reply.started":"2022-02-20T18:05:44.602564Z","shell.execute_reply":"2022-02-20T18:05:44.618014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlations using heatmap\nNo multicollinearity was detected. The highest correlation between features was V7 and Amount which had a 0.39730 correlation.","metadata":{}},{"cell_type":"code","source":"def printHeat():\n    corr = df.corr()\n    #print(corr)\n    highly_corr_features = corr.index[abs(corr[\"Class\"])>0.2]\n    plt.figure(figsize=(10,10))\n    heat = sns.heatmap(data[highly_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n    top10=corr[\"Class\"].sort_values(ascending=False).head(10).round(4)\n    print(heat)\n    #print(top10) # top ten correlations\n\n#print correlation between features\ndef printHighCorr(df, features, threshold=0.2):\n    print(\"Highly correlated variables above: \", threshold)\n    corr_df = df[features].corr() # get correlations\n    correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n    correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n    s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n    correlation_df={}\n    if s_corr_list == []:\n        print(\"There are no highly correlated features with correlation above\", threshold)\n    else:\n        for v, i, j in s_corr_list:\n            correlation_df[corr_df.index[i] +\" and \"+ corr_df.columns[j]]= v\n        correlation_df=pd.DataFrame(correlation_df,index=['Correlation']).round(4)\n    return  correlation_df.T.sort_values(by='Correlation',ascending=False)\n\n#Turn functions on or off below:\nprintHeat() \nprintHighCorr(data,data.columns).style.set_properties(**{'background-color': 'black','color': 'white'})","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:05:44.6219Z","iopub.execute_input":"2022-02-20T18:05:44.622152Z","iopub.status.idle":"2022-02-20T18:05:46.794295Z","shell.execute_reply.started":"2022-02-20T18:05:44.622126Z","shell.execute_reply":"2022-02-20T18:05:46.793271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Data\nUse 70% of the train data to predict the accuracy of the remaining 30% of the test data.","metadata":{}},{"cell_type":"code","source":"X=df.drop('Class', axis=1)\ny=df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:05:46.795667Z","iopub.execute_input":"2022-02-20T18:05:46.795934Z","iopub.status.idle":"2022-02-20T18:05:46.932225Z","shell.execute_reply.started":"2022-02-20T18:05:46.795904Z","shell.execute_reply":"2022-02-20T18:05:46.931256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### Gradient Boost with Feature Importance","metadata":{"execution":{"iopub.status.busy":"2022-02-11T21:03:12.342338Z","iopub.execute_input":"2022-02-11T21:03:12.343184Z","iopub.status.idle":"2022-02-11T21:03:12.347636Z","shell.execute_reply.started":"2022-02-11T21:03:12.343138Z","shell.execute_reply":"2022-02-11T21:03:12.34641Z"}}},{"cell_type":"code","source":"#GRADIENT BOOST REGRESSION:\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\nfrom sklearn import ensemble\n\nparams = {\n \"n_estimators\": 2, \"max_depth\": 4, \"min_samples_split\": 5, \"learning_rate\": 0.01,\n}\n\nreg = ensemble.GradientBoostingRegressor(**params)\nreg.fit(X_train, y_train)\n\ny_pred = reg.predict(X_test)\ngbr_r2 = r2_score(y_test, y_pred).round(4) \nprint(\"Gradient boosting regression r2: \", gbr_r2) \n\nmse = mean_squared_error(y_test, reg.predict(X_test))\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, y_pred), 4 ) \nprint(\"Gradient boosting regression AUC: \", auc )\n\n#FEATURE IMPORTANCE:\ncols=X.columns\nfeature_importance = reg.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + 0.5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align=\"center\")\nplt.yticks(pos, np.array(cols)[sorted_idx])\nplt.title(\"Feature Importance (MDI)\")\n\nresult = permutation_importance(\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\nplt.subplot(1, 2, 2)\nplt.boxplot(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=np.array(cols)[sorted_idx],\n)\nplt.title(\"Permutation Importance (test set)\")\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:05:46.933529Z","iopub.execute_input":"2022-02-20T18:05:46.933792Z","iopub.status.idle":"2022-02-20T18:06:03.627181Z","shell.execute_reply.started":"2022-02-20T18:05:46.933763Z","shell.execute_reply":"2022-02-20T18:06:03.626211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression\nAUC for linear regression is:  0.9844 <br>\nLinear regression r2 score:  0.5375 <br>\nLinear Accuracy:  0.5375 <br>\n0.4573  linear regression cross validate mean <br>","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:17:59.091558Z","iopub.execute_input":"2022-02-20T18:17:59.09185Z","iopub.status.idle":"2022-02-20T18:17:59.09661Z","shell.execute_reply.started":"2022-02-20T18:17:59.091821Z","shell.execute_reply":"2022-02-20T18:17:59.095616Z"}}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n\n#Fit and predict:\nlrModel = LinearRegression()\nlrModel.fit(X_train, y_train)\nlrPredict = lrModel.predict(X_test)\n\n# plt.scatter(y_test, predictions)\nplt.hist(y_test - lrPredict)\n\n#Linear Metrics:\nauc = round( metrics.roc_auc_score(y_test, lrPredict), 4 ) \nr2 = r2_score(y_test, lrPredict).round(4) \nprint(\"AUC for linear regression is: \", auc)\nprint(\"Linear regression r2 score: \", r2)\n\n#CROSS VALIDATE TEST RESULTS:\nlr_score = lrModel.score(X_test, y_test).round(4)  # train test \nprint(\"Linear Accuracy: \", lr_score)\nlr_cv = cross_validate(lrModel, X, y, cv = 5, scoring= 'r2')\nlr_cvMean=lr_cv['test_score'].mean().round(4)\nprint(lr_cvMean, \" linear regression cross validate mean\")\n\ndef linearReports():\n    print(model.coef_)    \n    print(model.intercept_)\n    print(classification_report(y_test_data, lrPredict))\n    print(confusion_matrix(y_test_data, lrPredict))\n    metrics.mean_absolute_error(y_test, lrPredict)\n    np.sqrt(metrics.mean_squared_error(y_test, lrPredict))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:55:48.455567Z","iopub.execute_input":"2022-02-20T18:55:48.456100Z","iopub.status.idle":"2022-02-20T18:55:50.727285Z","shell.execute_reply.started":"2022-02-20T18:55:48.456036Z","shell.execute_reply":"2022-02-20T18:55:50.726434Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\nAUC for logistic regression is:  0.905","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogReg = LogisticRegression(solver='liblinear') #solver param gets rid of encoder error\n\n#Train the model and create predictions\nlogReg.fit(X_train, y_train)\nlogPredict = logReg.predict_proba(X_test)[::,1]\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, logPredict), 4 ) \nprint(\"AUC for logistic regression is: \", auc)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:58:37.102505Z","iopub.execute_input":"2022-02-20T18:58:37.102807Z","iopub.status.idle":"2022-02-20T18:58:40.003238Z","shell.execute_reply.started":"2022-02-20T18:58:37.102778Z","shell.execute_reply":"2022-02-20T18:58:40.002230Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### K-Nearest Neigbors\n* KNN model AUC is:  0.9844","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef knnErorr():\n    print(\"Selecting an optimal K value:\")\n    error_rates = []\n    for i in range(1, 6, 2): #Must be an odd number to break a tie\n        new_model = KNeighborsClassifier(n_neighbors = i)\n        new_model.fit(X_train, y_train)\n        new_predictions = new_model.predict(X_test)\n        error_rates.append(np.mean(new_predictions != y_test))\n\n    plt.figure(figsize=(16,12))\n    plt.plot(error_rates)\n\n#Train the model and make predictions:\nknn = KNeighborsClassifier(n_neighbors =1) \nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, y_pred_proba), 4 ) \nprint(\"KNN model AUC is: \", auc)\n\n#Additional Info:\ndef knnReports():\n    acc = metrics.accuracy_score(y_test_data, knnPredict)\n    print(confusion_matrix(y_test, knnPredict))\n    print(classification_report(y_test, knnPredict))\n    print(confusion_matrix(y_test, knnPredict))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T18:52:26.627536Z","iopub.execute_input":"2022-02-20T18:52:26.627804Z","iopub.status.idle":"2022-02-20T18:52:32.955576Z","shell.execute_reply.started":"2022-02-20T18:52:26.627776Z","shell.execute_reply":"2022-02-20T18:52:32.954581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Resources\n1. https://www.kaggle.com/jdelamorena/recall-97-by-using-undersampling-neural-network\n2. https://www.kaggle.com/gpreda/credit-card-fraud-detection-predictive-models","metadata":{}}]}