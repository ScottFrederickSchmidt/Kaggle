{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Risk Morgage Loans\nThe data is provided by [Home Credit](https://www.homecredit.net/about-us.asp), who provides lines of credit (loans) to the unbanked population\". There are 307,511 rows with 122 columns. \n\nEach SK_ID_CURR in the test set, will predict a probability for the TARGET variable. The final prediction file should contain a header and have the following format:\nSK_ID_CURR,TARGET <br/>\n100001,0.1 <br/>\n100005,0.9 <br/>\n100013,0.2 <br/>\n\nUsing different models using AUC found the following:\n* Linear regression is:  0.7327\n* Logistic regression is: 0.6145\n\n## Seven Datasets Summary\nOriginal dataset csv files can be found on [Kaggle](https://www.kaggle.com/c/home-credit-default-risk). The columns with first five rows will be shown below to view whenever a dataset is used. Therefore, one will not have to download the csv files. There are seven sources of data for this project which will be briefly\ndescribed below:\n* Train.csv: This is the most important dataset with 307,511 rows which are house data. There are 106 column features describing houses such as square feet and year built. The column TARGET column is an important feature to discuss. A 1 in this row means the loan struggled to payback. A 0 means the loan was did not default. Some of the features will need to be encoded numerical to test if they have high feature importance.\n* bureau.csv: Other previous credit data from other financial institutions. \n* bureau_balance.csv: Monthly bureau previous credits.\n* brevious_application.csv: Previous appliation loans.\n* POS_CASH_BALANCE.csv: Monthly data about previous cash loans. \n* credit_card_balance.csv: Monthly credit card data for clients with Home Credit.\n* installments_payment.csv: Payment history for previous loans.\n<br/> <br/>\n\n## View Train.csv Data\nThe training dataset is the most important dataset with over three-hundred thousand house prices that will be predicted at the very using using the best metrics predictive models with reduced error. The first five rows of the train.csv file will be shown below.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom statistics import mean\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\n#sample=r'/kaggle/input/home-credit-default-risk/sample_submission.csv'\n#cash=r'/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv'\n#info='/kaggle/input/home-credit-default-risk/HomeCredit_columns_description.csv'\n#app=r'/kaggle/input/home-credit-default-risk/previous_application.csv'\n#cc=r'/kaggle/input/home-credit-default-risk/credit_card_balance.csv'\n#install=r'/kaggle/input/home-credit-default-risk/installments_payments.csv'\nbureau_balance=r'/kaggle/input/home-credit-default-risk/bureau_balance.csv'\ntrain=r'/kaggle/input/home-credit-default-risk/application_train.csv'\ntest=r'/kaggle/input/home-credit-default-risk/application_test.csv'\ndata=pd.read_csv(train) # (307511, 122)\ntest=pd.read_csv(test)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:29:12.236599Z","iopub.execute_input":"2022-02-18T01:29:12.237093Z","iopub.status.idle":"2022-02-18T01:29:22.365297Z","shell.execute_reply.started":"2022-02-18T01:29:12.236965Z","shell.execute_reply":"2022-02-18T01:29:22.364202Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Examine TARGET column\nHow many loans were not repaid? In train.csv a 0 stands for repaid and 1 stands for payment difficulties. The percent of loans that defauled was 0.081. This is somewhat unbalanced data so we must be careful when selecting what metrics to use to analyze the data. In addition, we must consider other data files for feature importance.","metadata":{}},{"cell_type":"code","source":"temp=data['TARGET'].value_counts()\nprint(temp)\npaid=temp[0]\nnotPaid=temp[1]\ndefault=round(notPaid/(paid+notPaid),3)\nprint(\"Percent of loans that defauled: \", default)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:29:22.367726Z","iopub.execute_input":"2022-02-18T01:29:22.368143Z","iopub.status.idle":"2022-02-18T01:29:22.389447Z","shell.execute_reply.started":"2022-02-18T01:29:22.368091Z","shell.execute_reply":"2022-02-18T01:29:22.388186Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Find missing values\nToo many missing values on a column will get the colunmn removed. Since there are 60 numeric columns with missing data, we need to interpret the Buraeu to find feature importance in order to engineer which columns are most worth keeping.","metadata":{}},{"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf = data.select_dtypes(include=numerics) # (307511, 106)\n\n#search for columns with missing values:\ndef findNA():\n    print(\"Missing data by column as a percent:\")\n    findNA=df.isnull().sum().sort_values(ascending=False)/len(data)\n    findNA.head(60)\n#findNA() ","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:29:22.391622Z","iopub.execute_input":"2022-02-18T01:29:22.391898Z","iopub.status.idle":"2022-02-18T01:29:22.496012Z","shell.execute_reply.started":"2022-02-18T01:29:22.391869Z","shell.execute_reply":"2022-02-18T01:29:22.495044Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Remove columns and Fill Missing Values","metadata":{}},{"cell_type":"code","source":"number=20 #remove col with  or more missing values\ndf = df[df.isnull().sum(axis=1) <= number] \ndf= df.fillna(df.mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:29:22.498295Z","iopub.execute_input":"2022-02-18T01:29:22.498559Z","iopub.status.idle":"2022-02-18T01:29:22.761914Z","shell.execute_reply.started":"2022-02-18T01:29:22.498529Z","shell.execute_reply":"2022-02-18T01:29:22.761085Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Heat Map Correlations and Multicollinearity\nThere is no major multicollinearity. In fact, there are not many correlated variables. The following heatmap is set for correlations above .05 because there are so few variables that are highly correlated.","metadata":{}},{"cell_type":"code","source":"def printHeat():\n    corr = df.corr()\n    #print(corr)\n    y='TARGET'\n    highly_corr_features = corr.index[abs(corr[y])>0.05]\n    plt.figure(figsize=(10,10))\n    heat = sns.heatmap(df[highly_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n    top10=corr[y].sort_values(ascending=False).head(10)\n    print(heat)\n    print(\"Top 10 Correlations:\\n\", top10) # top ten correlations\nprintHeat()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:56:23.238079Z","iopub.execute_input":"2022-02-18T01:56:23.238516Z","iopub.status.idle":"2022-02-18T01:56:27.887959Z","shell.execute_reply.started":"2022-02-18T01:56:23.238476Z","shell.execute_reply":"2022-02-18T01:56:27.887104Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Split Data\nSplit the data set into training data and test data. TARGET will always be Y since it is the independent variable. A 1 is a troubled loan while a 0 equals a not distressed loan. ","metadata":{}},{"cell_type":"code","source":"X=df.drop('TARGET', axis=1)\ny=df['TARGET'] #indepdent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:29:22.772199Z","iopub.execute_input":"2022-02-18T01:29:22.772436Z","iopub.status.idle":"2022-02-18T01:29:22.942967Z","shell.execute_reply.started":"2022-02-18T01:29:22.772407Z","shell.execute_reply":"2022-02-18T01:29:22.941955Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Booster and Feature Importance\nThe amount of annuity and days_birth (age) are the two most highly correlated features. However, since this is unbalanced data there do not seem to be many feature importance to begin with.","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n\nparams = {\n \"n_estimators\": 5, \"max_depth\": 4, \"min_samples_split\": 5, \"learning_rate\": 0.01,\n}\n\nreg = ensemble.GradientBoostingRegressor(**params)\nreg.fit(X_train, y_train)\n\ny_pred = reg.predict(X_test)\ngbr_r2 = r2_score(y_test, y_pred).round(4) \nprint(\"Gradient boosting regression r2: \", gbr_r2) \n\nmse = mean_squared_error(y_test, reg.predict(X_test))\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n\n#FEATURE IMPORTANCE:\nnum=10 # How many features?\ncols=X.columns\nfeature_importance = reg.feature_importances_[:num]\nsorted_idx = np.argsort(feature_importance)[:num]\npos = np.arange(sorted_idx.shape[0]) + 0.5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align=\"center\")\nplt.yticks(pos, np.array(cols)[sorted_idx])\nplt.title(\"Feature Importance (MDI)\")","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:29:22.944209Z","iopub.execute_input":"2022-02-18T01:29:22.944438Z","iopub.status.idle":"2022-02-18T01:29:31.101894Z","shell.execute_reply.started":"2022-02-18T01:29:22.944409Z","shell.execute_reply":"2022-02-18T01:29:31.100994Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\nAUC for logistic regression is:  0.6145.\n\n The c paramter in logistic regression model by definition is the following: \"Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization\". Using 1 the default value for C or putting C at .01 did not change the AUC for the logistic regression.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogReg = LogisticRegression(solver='liblinear') #solver param gets rid of encoder error\n\n#Train the model and create predictions\nlogReg.fit(X_train, y_train)\n\n#use model to predict probability that given y value is 1:\ny_pred_proba = logReg.predict_proba(X_test)[::,1]\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, y_pred_proba), 4 ) \nprint(\"AUC for logistic regression is: \", auc)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:48:45.996238Z","iopub.execute_input":"2022-02-18T02:48:45.998348Z","iopub.status.idle":"2022-02-18T02:48:53.136102Z","shell.execute_reply.started":"2022-02-18T02:48:45.998301Z","shell.execute_reply":"2022-02-18T02:48:53.135020Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression\nDue to small Y indepdent variables AUC is the more accurate metric than r_squared. Since the linear regression, accuracy, and cross validate are all near .045 it seems there is no sign of overfitting.\n\nAUC for linear regression is:  0.7327 <br/>\nLinear regression r2 score:  0.0475 <br/>\nAccuracy:  0.0475 <br/>\n0.0489  linear regression cross validate mean <br/>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n\nlrModel = LinearRegression()\nlrModel.fit(X_train, y_train)\n#print(model.coef_)    #print(model.intercept_)\n\n#Generate Predictions:\npredictions = lrModel.predict(X_test)\n\n# plt.scatter(y_test, predictions)\nplt.hist(y_test - predictions)\n\n#Performance measurement:\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, r2_score\n#print(classification_report(y_test_data, predictions))\n#print(confusion_matrix(y_test_data, predictions))\n\nmetrics.mean_absolute_error(y_test, predictions)\nnp.sqrt(metrics.mean_squared_error(y_test, predictions))\n\n#use model to predict probability that given y value is 1:\n#y_pred_proba = lrModel.predit(X_test)[::,1]\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, predictions), 4 ) \nprint(\"AUC for linear regression is: \", auc)\n\n#use model to predict probability that given y value is 1:\ny_pred_proba = lrModel.predict(X_test)\nr2 = r2_score(y_test, y_pred_proba).round(4) \nprint(\"Linear regression r2 score: \", r2)\n\n#CROSS VALIDATE TEST RESULTS:\nlr_score = lrModel.score(X_test, y_test).round(4)  # train test \nprint(\"Accuracy: \", lr_score)\nlr_cv = cross_validate(lrModel, X, y, cv = 5, scoring= 'r2')\nlr_cvMean=lr_cv['test_score'].mean().round(4)\nprint(lr_cvMean, \" linear regression cross validate mean\")","metadata":{"execution":{"iopub.status.busy":"2022-02-18T03:33:47.637797Z","iopub.execute_input":"2022-02-18T03:33:47.638111Z","iopub.status.idle":"2022-02-18T03:33:52.778896Z","shell.execute_reply.started":"2022-02-18T03:33:47.638080Z","shell.execute_reply":"2022-02-18T03:33:52.778271Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n## View Buraeu Data \nThe Buraeu data has [1716428 rows x 17 columns]. Three columns were categorical so they get removed.\nThen an additional four columns had lots of missing data, more than 80% so they are deleted.\nFinally, we remove a small portion of missing values just to get a general analysis of the missing data.\nThe goal is to use this additional information outside of the train set to try to find feature importance. \n'''\n\nburaeuData=r'/kaggle/input/home-credit-default-risk/bureau.csv'\nburaeuDF=pd.read_csv(buraeuData) #[1716428 rows x 17 columns]\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nburaeuDF = buraeuDF.select_dtypes(include=numerics) #(1716428, 14)\n# Six columns have missing values:\nbNA=buraeuDF.isnull().sum().sort_values(ascending=False)/len(buraeuDF) \nburaeuDF=buraeuDF.dropna(thresh=0.8*len(buraeuDF), axis=1) #(1716428, 10)\nburaeuDF = buraeuDF.dropna() #(1376391, 10)\nburaeuDF.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:29:34.085900Z","iopub.status.idle":"2022-02-18T01:29:34.086402Z","shell.execute_reply.started":"2022-02-18T01:29:34.086137Z","shell.execute_reply":"2022-02-18T01:29:34.086164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Resources\n1. https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction","metadata":{}}]}