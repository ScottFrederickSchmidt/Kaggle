{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Store Sales \n#### Use time series forecating and machine learning to predict grocery sales. \n\nPredict sales for the thousands of product families sold at Favorita stores located in Ecuador. The training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers. \n\nOriginal dataset can be found here on Kaggle:\nhttps://www.kaggle.com/c/store-sales-time-series-forecasting/code?competitionId=29781&sortBy=dateRun&tab=profile","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom statistics import mean\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\nimport sklearn.metrics as metrics\n\noil=r'/kaggle/input/store-sales-time-series-forecasting/oil.csv'\nsample=r'/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv'\nevent=r'/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv'\nstores=r'/kaggle/input/store-sales-time-series-forecasting/stores.csv'\ntrain=r'/kaggle/input/store-sales-time-series-forecasting/train.csv'\ntest=r'/kaggle/input/store-sales-time-series-forecasting/test.csv'\ntransaction=r'/kaggle/input/store-sales-time-series-forecasting/transactions.csv'","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:24:20.816022Z","iopub.execute_input":"2022-02-21T22:24:20.816462Z","iopub.status.idle":"2022-02-21T22:24:20.825754Z","shell.execute_reply.started":"2022-02-21T22:24:20.816412Z","shell.execute_reply":"2022-02-21T22:24:20.824754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Dataset \nThe training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers.","metadata":{}},{"cell_type":"code","source":"trainDF=pd.read_csv(train)\ntrainDF.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:24:20.827537Z","iopub.execute_input":"2022-02-21T22:24:20.828547Z","iopub.status.idle":"2022-02-21T22:24:23.052432Z","shell.execute_reply.started":"2022-02-21T22:24:20.828498Z","shell.execute_reply":"2022-02-21T22:24:23.051422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transactions \nThe original dataset is 83488 rows Ã— 3 columns.","metadata":{}},{"cell_type":"code","source":"tDF=pd.read_csv(transaction)\ntDF.head()\ntDF.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False) #no missing data\n\n#Feature three new columns with year, month, and day:\ntDF['year']=tDF['date'].apply(lambda x: int(str(x)[:4])) #first four\ntDF['month']=tDF['date'].apply(lambda x: int(str(x)[5:7])) #five and six\ntDF['day']=tDF['date'].apply(lambda x: int(str(x)[-2:])) # last two\n\nprint(tDF['day'].unique()) \n#[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31]\nprint(tDF['month'].unique()) # [ 1  2  3  4  5  6  7  8  9 10 11 12]\nprint(tDF['year'].unique()) # [2013 2014 2015 2016 2017]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:28:05.467259Z","iopub.execute_input":"2022-02-21T22:28:05.467605Z","iopub.status.idle":"2022-02-21T22:28:05.78725Z","shell.execute_reply.started":"2022-02-21T22:28:05.46757Z","shell.execute_reply":"2022-02-21T22:28:05.786217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:24:23.370506Z","iopub.execute_input":"2022-02-21T22:24:23.370879Z","iopub.status.idle":"2022-02-21T22:24:23.381001Z","shell.execute_reply.started":"2022-02-21T22:24:23.370832Z","shell.execute_reply":"2022-02-21T22:24:23.3797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The date column is currently an object but needs to be chnaged to a datetime datatype column. This will help later when we use the create_time_features later. Dates are initially as 2013-01-02 format so wo will keep that consistent. Missing values must be deleted one last time in the event there was an coerce error that occurs. Finally, we must transform each date to a datetime that can be regonized by using .dt for the create_time_features function to work. Here is a good SO article on how to solve this issue:\nhttps://stackoverflow.com/questions/56698521/can-only-use-dt-accessor-with-datetimelike-values/56698574","metadata":{}},{"cell_type":"code","source":"tDF['date'] = pd.to_datetime(tDF['date'], errors='coerce', format = '%Y-%m-%d')\nprint(tDF)\n\nyears=tDF.groupby('date')[:4]['transactions'].sum()\n\ntDF.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False) #Above line could cause NA\n\n\ntDF['year'] = tDF['date'].dt.year\ntDF['dayofyear'] = tDF['date'].dt.dayofyear\ntDF['dayofweek'] = tDF['date'].dt.dayofweek\nyear=tDF.groupby('year')['transactions'].sum()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:24:23.382444Z","iopub.execute_input":"2022-02-21T22:24:23.383381Z","iopub.status.idle":"2022-02-21T22:24:23.437217Z","shell.execute_reply.started":"2022-02-21T22:24:23.383347Z","shell.execute_reply":"2022-02-21T22:24:23.436269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multivariate time series forecasting\nHere is a full function for all the new columns you can make from a datetime. In this specific sales case, we will only be needing month, year, and day since time was not provided.","metadata":{}},{"cell_type":"code","source":"import datetime as dt\nfrom datetime import datetime\n\n# ADD time features to our model:\ndef create_time_features(df, target=None):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    df['date'] = df.index\n    #df['hour'] = df['date'].dt.hour\n    #df['dayofweek'] = df['date'].dt.dayofweek\n    #df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    #df['sin_day'] = np.sin(df['dayofyear'])\n    #df['cos_day'] = np.cos(df['dayofyear'])\n    #df['dayofmonth'] = df['date'].dt.day\n    #df['weekofyear'] = df['date'].dt.weekofyear\n    X = df.drop(['date'], axis=1)\n    if target:\n        y = df[target]\n        X = X.drop([target], axis=1)\n        return X, y\n    \n    return X","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:24:23.43837Z","iopub.status.idle":"2022-02-21T22:24:23.438743Z","shell.execute_reply.started":"2022-02-21T22:24:23.438535Z","shell.execute_reply":"2022-02-21T22:24:23.438557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nX_train_df, y_train = create_time_features(tDF)\nX_test_df, y_test = create_time_features(tDF)\n\nscaler = StandardScaler()\nscaler.fit(X_train_df)  # No cheating, never scale on the training+test!\nX_train = scaler.transform(X_train_df)\nX_test = scaler.transform(X_test_df)\n\nX_train_df = pd.DataFrame(X_train, columns=X_train_df.columns)\nX_test_df = pd.DataFrame(X_test, columns=X_test_df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:24:23.43985Z","iopub.status.idle":"2022-02-21T22:24:23.440211Z","shell.execute_reply.started":"2022-02-21T22:24:23.440019Z","shell.execute_reply":"2022-02-21T22:24:23.440045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stores Dataset","metadata":{}},{"cell_type":"code","source":"stores=pd.read_csv(stores)\nstores.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:24:23.441321Z","iopub.status.idle":"2022-02-21T22:24:23.441661Z","shell.execute_reply.started":"2022-02-21T22:24:23.441495Z","shell.execute_reply":"2022-02-21T22:24:23.441516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Events","metadata":{}},{"cell_type":"code","source":"event=pd.read_csv(event)\nevent.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:24:23.443137Z","iopub.status.idle":"2022-02-21T22:24:23.443453Z","shell.execute_reply.started":"2022-02-21T22:24:23.443283Z","shell.execute_reply":"2022-02-21T22:24:23.443304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearMonth'] = pd.to_datetime(df['Date']).apply(lambda x: '{year}-{month}'.format(year=x.year, month=x.month))\nres = df.groupby('YearMonth')['Values'].sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T22:24:23.445207Z","iopub.status.idle":"2022-02-21T22:24:23.445549Z","shell.execute_reply.started":"2022-02-21T22:24:23.44537Z","shell.execute_reply":"2022-02-21T22:24:23.445401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\nMost of these resources are on Time Series:\n1. https://www.kaggle.com/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide\n2. https://www.kaggle.com/ilyakondrusevich/54-stores-54-models\n3. https://github.com/jiwidi/time-series-forecasting-with-python/blob/master/02-Forecasting_models.ipynb","metadata":{}}]}