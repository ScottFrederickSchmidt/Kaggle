'''
https://www.kaggle.com/scottfredschmidt/predicthouseprices

Real data with 79 explanatory variables describing aspects of residential homes in Ames, Iowa.
The final price of each home is predicted using:
1) Realative feature engineering,
2) Advanced regression techniques (random forest and gradient boosting).
'''
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

print("start")

data=pd.read_csv(r'C:\Users\sschm\Desktop\HousePrice/train.csv') #(1459, 80)

cols=['MSSubClass', 'LotArea', 'OverallQual','OverallCond','YearBuilt', 'MoSold', 
             'YrSold', 'MasVnrArea', 'BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', '1stFlrSF', '2ndFlrSF',
             'GrLivArea','WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch','3SsnPorch', 'ScreenPorch','PoolArea', 'SalePrice'
            ]

df = pd.DataFrame(data, columns=cols) 
df = df[df.isnull().sum(axis=1) <= 5] #remove col with 5 or more missing values
df = df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)
X=df.drop('SalePrice', axis=1)
y=df['SalePrice']

#Find Multicollinearity using VIF:
from statsmodels.stats.outliers_influence import variance_inflation_factor
def calc_VIF(x):
    vif= pd.DataFrame()
    vif['variables']=x.columns
    vif["VIF"]=[variance_inflation_factor(x.values,i) for i in range(x.shape[1])]
    return(vif)

#Find Multicollinearity using heat map:
def printHeat():
    corr = df.corr()
    #print(corr)
    highly_corr_features = corr.index[abs(corr["SalePrice"])>0.5]
    plt.figure(figsize=(10,10))
    heat = sns.heatmap(df[highly_corr_features].corr(),annot=True,cmap="RdYlGn")
    top10=corr["SalePrice"].sort_values(ascending=False).head(10)
    print(heat)
    print(top10)

def pltFigure():
    fig = plt.figure(figsize=(12,10))
    #GarageArea
    plt.subplot(321)
    sns.scatterplot(data=train, x='GarageArea', y="SalePrice")

#Sorting columns null values:
def findNA():
    findNA=df.isna().sum()  #check = df[''].value_counts()
    total=data.isnull().sum().sort_values(ascending=False)
    total=total.head(20)
#printHeat()  #will print heatmap and correlations
#calc_VIF(df[::-1]) #test for multicollinearity using VIF
#pltFigure() #plot figures


#Split the data set into training data and test data:
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from statistics import mean
​
x_train_data, x_test_data, y_train_data, y_test_data = train_test_split(X, y, test_size = 0.3, random_state=42)
​
lrModel = LinearRegression()
lrModel.fit(x_train_data, y_train_data)
#print(model.coef_)
#print(model.intercept_)
​
#Generate Predictions:
predictions = lrModel.predict(x_test_data)
​
# plt.scatter(y_test, predictions)
plt.hist(y_test_data - predictions)
​
#Performance measurement:
import sklearn.metrics as metrics
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, r2_score
#print(classification_report(y_test_data, predictions))
#print(confusion_matrix(y_test_data, predictions))
​
metrics.mean_absolute_error(y_test_data, predictions)
metrics.mean_squared_error(y_test_data, predictions)
np.sqrt(metrics.mean_squared_error(y_test_data, predictions))
​
#use model to predict probability that given y value is 1:
y_pred_proba = lrModel.predict(x_test_data)
​
#calculate AUC
#auc = round( metrics.roc_auc_score(y_test_data, y_pred_proba, multi_class='ovr'), 4 ) 
#print(auc, " AUC ", )
​
r2 = r2_score(y_test_data, y_pred_proba).round(4) 
print("r2 score: ", r2)
​
#Cross Validation Test Results:
lr_score = lrModel.score(X_test, y_test).round(4)  # train test 
print("Accuracy: ", lr_score)
lr_cv = cross_validate(lrModel, X, y, cv = 5, scoring= 'r2')
#print("Cross-validation results: ", lr_cv)
#print("R2 Cross-validation: ", np.array(list(lr_cv.values())).mean().round(4) ) 
​
#RIDGE REGRESSION:
ridge = Ridge(alpha = 1)  # sets alpha to a default value as baseline  
ridge.fit(X_train, y_train)
​
ridge_cv = cross_validate(ridge, X, y, cv = 5, scoring = 'r2')
print ("Cross-validation results: ", ridge_cv)
print ("Ridge Regression R2: ", ridge_cv.mean())

#LASSO REGRESSION:
lasso = Lasso(alpha = .1, normalize=True)  # sets alpha to almost zero as baseline
lasso.fit(x_train_data, y_train_data)
lasso_cv = cross_validate(lasso, X, y, cv = 5, scoring = 'r2')
lasso_cvMean=lasso_cv['test_score'].mean().round(4)
#print ("Lasso Regression R2: ", lasso_cvMean)

'''
Create a predition using random forest
Splitting the dataset into the Training set and Test set
'''
#RANDOM FOREST MODEL:
from sklearn.ensemble import RandomForestRegressor

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)

#TEST TO SEE WHAT n_estimators value produces the least amount of MAE error:
dMAE={} #dictionary of n_estimators as key and MAE as value:
def checkMAE():
    for n in range(100, 1401, 100):
        regressor = RandomForestRegressor(n_estimators=n, random_state = 0)
        regressor.fit(X_train, y_train)
        y_pred = regressor.predict(X_test)
        MAE=metrics.mean_absolute_error(y_test, y_pred).round(2)
        dMAE[n]=MAE
        #print("n_estimates: ", n,  '  Mean Absolute Error:', MAE)

    dMAE=sorted(((v, k) for k, v in dMAE.items()), reverse=False)
    #print(dMAE) #[(18573.45, 400), (18632.04, 200), (18636.62, 500), (18644.81, 300), (18651.96, 600),

#Final R2 value using 400 as n_estimators
num=400 #lowest MAE in sorted dic
regressor = RandomForestRegressor(n_estimators=num, random_state = 0)
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
rf_r2 = r2_score(y_test, y_pred).round(4) 
print("Random forest r2: ", rf_r2) #Random forest r2:  0.8921

#GRADIENT BOOST REGRESSION:
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, ensemble
from sklearn.inspection import permutation_importance
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=13
)

params = {
    "n_estimators": 500,
    "max_depth": 4,
    "min_samples_split": 5,
    "learning_rate": 0.01,
}

reg = ensemble.GradientBoostingRegressor(**params)
reg.fit(X_train, y_train)

mse = mean_squared_error(y_test, reg.predict(X_test))
print("The mean squared error (MSE) on test set: {:.4f}".format(mse))

feature_importance = reg.feature_importances_
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + 0.5
fig = plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.barh(pos, feature_importance[sorted_idx], align="center")
plt.yticks(pos, np.array(cols)[sorted_idx])
plt.title("Feature Importance (MDI)")

result = permutation_importance(
    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2
)
sorted_idx = result.importances_mean.argsort()
plt.subplot(1, 2, 2)
plt.boxplot(
    result.importances[sorted_idx].T,
    vert=False,
    labels=np.array(cols)[sorted_idx],
)
plt.title("Permutation Importance (test set)")
fig.tight_layout()
plt.show()




'''
{250: 18632.242, 500: 18636.621, 750: 18680.353, 1000: 18696.357, 1250: 18674.225}
[(18696.357, 1000), (18680.353, 750), (18674.225, 1250), (18636.621, 500), (18632.242, 250)]
done

TOP CORRELATIONS to SalesPrice:
OverallQual    0.789997
GrLivArea      0.710080
TotalBsmtSF    0.612971
1stFlrSF       0.606849
YearBuilt      0.522896
MasVnrArea     0.477493
BsmtFinSF1     0.383977
WoodDeckSF     0.324650
2ndFlrSF       0.322710
'''
