{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Risk Morgage Loans\nUse machine learning to detect credit risk on homes and the owner's ability repay the loan.\n\nEach SK_ID_CURR in the test set, will predict a probability for the TARGET variable. The final prediction file should contain a header and have the following format:\nSK_ID_CURR,TARGET <br />\n100001,0.1 <br />\n100005,0.9 <br />\n100013,0.2 <br />\n\n## View Data\nDataset csv file can be found here: https://www.kaggle.com/c/home-credit-default-risk  <br> \nThere are 307511 rows with 122 columns. The columns and first five rows will be shown below to view. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom statistics import mean\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\n#sample=r'/kaggle/input/home-credit-default-risk/sample_submission.csv'\n#bureau=r'/kaggle/input/home-credit-default-risk/bureau_balance.csv'\n#cash=r'/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv'\n#info='/kaggle/input/home-credit-default-risk/HomeCredit_columns_description.csv'\n#app=r'/kaggle/input/home-credit-default-risk/previous_application.csv'\n#cc=r'/kaggle/input/home-credit-default-risk/credit_card_balance.csv'\n#install=r'/kaggle/input/home-credit-default-risk/installments_payments.csv'\ntrain=r'/kaggle/input/home-credit-default-risk/application_train.csv'\ntest=r'/kaggle/input/home-credit-default-risk/application_test.csv'\nburaeu=r'/kaggle/input/home-credit-default-risk/bureau.csv'\ndata=pd.read_csv(train) # (307511, 122)\ntest=pd.read_csv(test)\nprint(data.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T03:23:14.634484Z","iopub.execute_input":"2022-02-14T03:23:14.634843Z","iopub.status.idle":"2022-02-14T03:23:21.497968Z","shell.execute_reply.started":"2022-02-14T03:23:14.634815Z","shell.execute_reply":"2022-02-14T03:23:21.497331Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Find missing values\nToo many missing values on a column will get the colunmn removed. Since there are 60 numeric columns with missing data, we need to interpret the Buraeu to find feature importance in order to engineer which columns are most worth keeping.","metadata":{}},{"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf = data.select_dtypes(include=numerics) # (307511, 106)\n\n#search for columns with missing values:\ndef findNA():\n    print(\"Missing data by column as a percent:\")\n    findNA=df.isnull().sum().sort_values(ascending=False)/len(data)\n    print(findNA.head(60))\n#findNA() ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T03:23:21.499519Z","iopub.execute_input":"2022-02-14T03:23:21.499734Z","iopub.status.idle":"2022-02-14T03:23:21.559679Z","shell.execute_reply.started":"2022-02-14T03:23:21.499708Z","shell.execute_reply":"2022-02-14T03:23:21.558645Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Remove columns and Fill Missing Values","metadata":{}},{"cell_type":"code","source":"df = df[df.isnull().sum(axis=1) <= 20] #remove col with 5 or more missing values\ndf= df.fillna(df.mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-14T03:23:21.561230Z","iopub.execute_input":"2022-02-14T03:23:21.561543Z","iopub.status.idle":"2022-02-14T03:23:21.751025Z","shell.execute_reply.started":"2022-02-14T03:23:21.561505Z","shell.execute_reply":"2022-02-14T03:23:21.749990Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Heat Map Correlations and Multicollinearity\nThere is no major multicollinearity. In fact, there are not many correlated variables. The following heatmap is set for correlations above .05 because there are so few variables that are highly correlated.","metadata":{}},{"cell_type":"code","source":"def printHeat():\n    corr = df.corr()\n    #print(corr)\n    y='TARGET'\n    highly_corr_features = corr.index[abs(corr[y])>0.05]\n    plt.figure(figsize=(10,10))\n    heat = sns.heatmap(df[highly_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n    top10=corr[y].sort_values(ascending=False).head(10)\n    print(heat)\n    print(\"Top 10 Correlations:\\n\", top10) # top ten correlations\n#printHeat()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T03:23:21.753115Z","iopub.execute_input":"2022-02-14T03:23:21.753383Z","iopub.status.idle":"2022-02-14T03:23:21.759971Z","shell.execute_reply.started":"2022-02-14T03:23:21.753346Z","shell.execute_reply":"2022-02-14T03:23:21.759431Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## View Buraeu Data for Feature Importance\nThe Buraeu data has [1716428 rows x 17 columns]\n\nbData=r'/kaggle/input/home-credit-default-risk/bureau.csv'\nbData=pd.read_csv(bData) #[1716428 rows x 17 columns]\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nbDF = bData.select_dtypes(include=numerics)\nprint(bDF.head())","metadata":{}},{"cell_type":"markdown","source":"### Split Data\nSplit the data set into training data and test data. Target will always be Y since it is the independent variable.","metadata":{}},{"cell_type":"code","source":"X=df.drop('TARGET', axis=1)\ny=df['TARGET']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T03:23:21.761506Z","iopub.execute_input":"2022-02-14T03:23:21.762171Z","iopub.status.idle":"2022-02-14T03:23:21.909899Z","shell.execute_reply.started":"2022-02-14T03:23:21.762140Z","shell.execute_reply":"2022-02-14T03:23:21.908605Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Booster and Feature Importance","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n\nparams = {\n \"n_estimators\": 5, \"max_depth\": 4, \"min_samples_split\": 5, \"learning_rate\": 0.01,\n}\n\nreg = ensemble.GradientBoostingRegressor(**params)\nreg.fit(X_train, y_train)\n\ny_pred = reg.predict(X_test)\ngbr_r2 = r2_score(y_test, y_pred).round(4) \nprint(\"Gradient boosting regression r2: \", gbr_r2) \n\nmse = mean_squared_error(y_test, reg.predict(X_test))\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n\n#FEATURE IMPORTANCE:\nnum=10 # How many features?\ncols=X.columns\nfeature_importance = reg.feature_importances_[:num]\nsorted_idx = np.argsort(feature_importance)[:num]\npos = np.arange(sorted_idx.shape[0]) + 0.5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align=\"center\")\nplt.yticks(pos, np.array(cols)[sorted_idx])\nplt.title(\"Feature Importance (MDI)\")","metadata":{"execution":{"iopub.status.busy":"2022-02-14T03:23:21.911700Z","iopub.execute_input":"2022-02-14T03:23:21.912038Z","iopub.status.idle":"2022-02-14T03:23:29.808315Z","shell.execute_reply.started":"2022-02-14T03:23:21.911989Z","shell.execute_reply":"2022-02-14T03:23:29.807350Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n\nlrModel = LinearRegression()\nlrModel.fit(X_train, y_train)\n#print(model.coef_)\n#print(model.intercept_)\n\n#Generate Predictions:\npredictions = lrModel.predict(X_test)\n\n# plt.scatter(y_test, predictions)\nplt.hist(y_test - predictions)\n\n#Performance measurement:\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, r2_score\n#print(classification_report(y_test_data, predictions))\n#print(confusion_matrix(y_test_data, predictions))\n\nmetrics.mean_absolute_error(y_test, predictions)\nnp.sqrt(metrics.mean_squared_error(y_test, predictions))\n\n#use model to predict probability that given y value is 1:\ny_pred_proba = lrModel.predict(X_test)\nr2 = r2_score(y_test, y_pred_proba).round(4) \nprint(\"Linear regression r2 score: \", r2)\n\n#CROSS VALIDATE TEST RESULTS:\nlr_score = lrModel.score(X_test, y_test).round(4)  # train test \nprint(\"Accuracy: \", lr_score)\nlr_cv = cross_validate(lrModel, X, y, cv = 5, scoring= 'r2')\nlr_cvMean=lr_cv['test_score'].mean().round(4)\nprint(lr_cvMean, \" linear regression cross validate mean\")\n\n#RIDGE REGRESSION:\nridge = Ridge(alpha = .5)  # sets alpha to a default value as baseline  \nridge.fit(X_train, y_train)\nridge_cv = cross_validate(ridge, X, y, cv = 5, scoring = 'r2')\nridge_cvMean=ridge_cv['test_score'].mean().round(4)\nprint (\"Ridge Regression R2: \", ridge_cvMean)\n\n#LASSO REGRESSION:\nlasso = Lasso(alpha = .1, normalize=True)  # sets alpha to almost zero as baseline\nlasso.fit(X_train, y_train)\nlasso_cv = cross_validate(lasso, X, y, cv = 5, scoring = 'r2')\nlasso_cvMean=lasso_cv['test_score'].mean().round(4)\nprint (\"Lasso Regression R2: \", lasso_cvMean)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T03:23:29.809578Z","iopub.execute_input":"2022-02-14T03:23:29.809838Z","iopub.status.idle":"2022-02-14T03:23:36.329384Z","shell.execute_reply.started":"2022-02-14T03:23:29.809803Z","shell.execute_reply":"2022-02-14T03:23:36.328361Z"},"trusted":true},"execution_count":8,"outputs":[]}]}