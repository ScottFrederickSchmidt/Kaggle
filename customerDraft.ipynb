{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customer Satisfaction\n### Business Problem Introduction\nFrom frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving.\n\nSantander Bank is asking Kagglers to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late.\n\nIn this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.\n\nThe original dataset can be found on Kaggle: <br> https://www.kaggle.com/c/santander-customer-satisfaction\n\n### Quick Summary\nSince the data is somewhat unbalanced, r_squared is not the best metric to be used. Therefore, we will use AUC (Area Under the Curve) as a metric:\n* Linear regression is:  0.7838\n* Logistic regression is:  0.5782\n\n--------------\n## View Train Data\nThe train data is the most important dataset in which we try to find trends on the TARGET column, Y indepdendent variable.","metadata":{"execution":{"iopub.status.busy":"2022-02-26T01:02:28.405424Z","iopub.execute_input":"2022-02-26T01:02:28.405674Z","iopub.status.idle":"2022-02-26T01:02:28.411147Z","shell.execute_reply.started":"2022-02-26T01:02:28.405647Z","shell.execute_reply":"2022-02-26T01:02:28.410434Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom statistics import mean\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\nimport sklearn.metrics as metrics\n\nsample=r'/kaggle/input/santander-customer-satisfaction/sample_submission.csv'\ntrain=r'/kaggle/input/santander-customer-satisfaction/train.csv'\ntest=r'/kaggle/input/santander-customer-satisfaction/test.csv'\n\ndata=pd.read_csv(train)\ndata.drop_duplicates() #no duplicates\nprint(data.shape) #(76020, 371)\ndata.head() ","metadata":{"execution":{"iopub.status.busy":"2022-02-28T00:28:05.000601Z","iopub.execute_input":"2022-02-28T00:28:05.001139Z","iopub.status.idle":"2022-02-28T00:28:09.615609Z","shell.execute_reply.started":"2022-02-28T00:28:05.001050Z","shell.execute_reply":"2022-02-28T00:28:09.614782Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Find Missing Values\nThe dataset is still structured as (76020, 371) meaning all the columns must have been numeric. There are no missing values. ","metadata":{}},{"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf = data.select_dtypes(include=numerics) \n\n#search for columns with missing values:\ndef findNA():\n    print(\"Missing data by column as a percent:\")\n    findNA=df.isnull().sum().sort_values(ascending=False)/len(df)\n    print(findNA.head())\nfindNA() ","metadata":{"execution":{"iopub.status.busy":"2022-02-28T00:28:09.617161Z","iopub.execute_input":"2022-02-28T00:28:09.617398Z","iopub.status.idle":"2022-02-28T00:28:09.739790Z","shell.execute_reply.started":"2022-02-28T00:28:09.617371Z","shell.execute_reply":"2022-02-28T00:28:09.739097Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Examine Target Column\nCustomer satisfaction is in 1 and 0. This means that we can use logistic regression to further analyze any trends in the data. ","metadata":{}},{"cell_type":"code","source":"target=df['TARGET'].unique()\nprint(target)\n\n#Check for unbalanced data:\ntarList=list(df['TARGET'])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T00:28:09.740834Z","iopub.execute_input":"2022-02-28T00:28:09.741046Z","iopub.status.idle":"2022-02-28T00:28:09.752914Z","shell.execute_reply.started":"2022-02-28T00:28:09.741020Z","shell.execute_reply":"2022-02-28T00:28:09.751963Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Identify Highly Correlated Features:\nFirst, we must remove highly correlated features that are above .80 correlation. Normally, creating a heapmap visualization is helpful but with 371 column features, will not be memory efficient unless using a supercomputer.","metadata":{}},{"cell_type":"code","source":"print(df.shape, \" before removing highly correlated variables.\")\n# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n\ndf=df.drop(to_drop, axis = 1)\nprint(df.shape, \" after removing highly correlated variables.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-28T00:28:09.754398Z","iopub.execute_input":"2022-02-28T00:28:09.754981Z","iopub.status.idle":"2022-02-28T00:28:37.985818Z","shell.execute_reply.started":"2022-02-28T00:28:09.754938Z","shell.execute_reply":"2022-02-28T00:28:37.984906Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X=df.drop('TARGET', axis=1)\ny=df['TARGET']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T00:28:37.988429Z","iopub.execute_input":"2022-02-28T00:28:37.988942Z","iopub.status.idle":"2022-02-28T00:28:38.155554Z","shell.execute_reply.started":"2022-02-28T00:28:37.988890Z","shell.execute_reply":"2022-02-28T00:28:38.154580Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\nAUC for logistic regression is:  0.5782.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogReg = LogisticRegression(solver='liblinear') #solver param gets rid of encoder error\n\n#Train the model and create predictions\nlogReg.fit(X_train, y_train)\nlogPredict = logReg.predict_proba(X_test)[::,1]\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, logPredict), 4 ) \nprint(\"AUC for logistic regression is: \", auc)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T00:28:38.157149Z","iopub.execute_input":"2022-02-28T00:28:38.157398Z","iopub.status.idle":"2022-02-28T00:28:40.511177Z","shell.execute_reply.started":"2022-02-28T00:28:38.157372Z","shell.execute_reply":"2022-02-28T00:28:40.510244Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression\nAUC for linear regression is:  0.7838","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n\n#Fit and predict:\nlrModel = LinearRegression()\nlrModel.fit(X_train, y_train)\nlrPredict = lrModel.predict(X_test)\n\n# plt.scatter(y_test, predictions)\nplt.hist(y_test - lrPredict)\n\n#Linear Metrics:\nauc = round( metrics.roc_auc_score(y_test, lrPredict), 4 ) \nr2 = r2_score(y_test, lrPredict).round(4) \nprint(\"AUC for linear regression is: \", auc)\nprint(\"Linear regression r2 score: \", r2)\n\n#CROSS VALIDATE TEST RESULTS:\nlr_score = lrModel.score(X_test, y_test).round(4)  # train test \nprint(\"Linear Accuracy: \", lr_score)\nlr_cv = cross_validate(lrModel, X, y, cv = 5, scoring= 'r2')\nlr_cvMean=lr_cv['test_score'].mean().round(4)\nprint(lr_cvMean, \" linear regression cross validate mean\")\n\ndef linearReports():\n    print(model.coef_)    \n    print(model.intercept_)\n    print(classification_report(y_test_data, lrPredict))\n    print(confusion_matrix(y_test_data, lrPredict))\n    metrics.mean_absolute_error(y_test, lrPredict)\n    np.sqrt(metrics.mean_squared_error(y_test, lrPredict))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T00:28:40.512869Z","iopub.execute_input":"2022-02-28T00:28:40.513472Z","iopub.status.idle":"2022-02-28T00:28:45.055141Z","shell.execute_reply.started":"2022-02-28T00:28:40.513425Z","shell.execute_reply":"2022-02-28T00:28:45.054556Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## References\n1. https://www.kaggle.com/rahulanand0070/feature-selection\n2. https://www.kaggle.com/solegalli/feature-selection-with-feature-engine","metadata":{}}]}