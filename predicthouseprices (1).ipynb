{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predict House Prices\nData has 79 explanatory variables describing aspects of residential homes in Ames, Iowa which can be downloaded [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/). Prediction modeling resulted in the following starting with the best:\n* Decision tree r-squared is 0.9276 when using the best max tree.\n* Gradient booster classification had the highest accuracy of 0.899 with a deviation of 0.031. \n* Random forest also had a very high r-squared value at  0.8905. \n* Linear regression produced only a 0.8002 result and 0.77850 for both lasso and ridge.\n\n-----------------\nFirst, data is cleaned such as checking how many missing values are in each columm. Too many missing data points will get a colulmn removed. An initial DataFrame with numeric values is analyzed. Later on, some catergorical values will be assigned a numeric value to be futher analyzed. The very last step will be to fill in any missing data points with the mean of that column. Deleting rows would be a bad idea since we need all rows to predict every house price.","metadata":{"_uuid":"c813f83c-e6be-4656-a5a6-1165d24b39f6","_cell_guid":"e8af0bdf-e21a-4644-a469-df7c673d90b6","trusted":true}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom statistics import mean\nfrom sklearn.metrics import mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata=pd.read_csv(r'/kaggle/input/house-prices-advanced-regression-techniques/train.csv')#(1459, 80) \ntest=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf = data.select_dtypes(include=numerics)\n\ndf['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'].drop(columns=['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'])\n#df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\ndf = df[df.isnull().sum(axis=1) <= 10] #remove col with 5 or more missing values\ndf= df.fillna(df.mean())\n\nX=df.drop('SalePrice', axis=1)\ny=df['SalePrice']\n\n#search for columns with missing values:\ndef findNA():\n    findNA=df.isna().sum() \n    total=data.isnull().sum().sort_values(ascending=False)\n    total=total.head(5)\n    print(\"Missing values per column: \")\n    print(total)\nfindNA()","metadata":{"_uuid":"21bbb438-4046-446a-be75-db717a15c4e4","_cell_guid":"48ad3ba7-645e-4809-b1c7-4a3ec0a963ce","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-31T01:25:16.226477Z","iopub.execute_input":"2022-01-31T01:25:16.226769Z","iopub.status.idle":"2022-01-31T01:25:16.314908Z","shell.execute_reply.started":"2022-01-31T01:25:16.226739Z","shell.execute_reply":"2022-01-31T01:25:16.314106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlations and Multicollinearity\nUsing a visual heatmap can display all correlations between features and detect multicollinearity. No major multicollinearity was detected other than first floor and basement floor with a .82 correlation. The square feet features were all combined into total square feet of the entire house. Variance inflation factor can also be calculated by calling the Python function by removing the comment on bottom.","metadata":{"_uuid":"a065e2cd-4e3b-410f-a924-9d22443297b1","_cell_guid":"5a695a7c-eeb5-458e-a9ab-8bebbf04b7ec","trusted":true}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ndef calc_VIF(x):\n    vif= pd.DataFrame()\n    vif['variables']=x.columns\n    vif[\"VIF\"]=[variance_inflation_factor(x.values,i) for i in range(x.shape[1])]\n    return(vif)\n\n#Find Multicollinearity using heat map:\ndef printHeat():\n    corr = df.corr()\n    #print(corr)\n    highly_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.5]\n    plt.figure(figsize=(10,10))\n    heat = sns.heatmap(df[highly_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n    top10=corr[\"SalePrice\"].sort_values(ascending=False).head(10)\n    print(heat)\n    #print(top10) # top ten correlations\n\n#print correlation between features\ndef printHighCorr(df, features, threshold=0.8):\n    print(\"Highly correlated variables above: \", threshold)\n    corr_df = df[features].corr() # get correlations\n    correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n    correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n    s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n    correlation_df={}\n    if s_corr_list == []:\n        print(\"There are no highly correlated features with correlation above\", threshold)\n    else:\n        for v, i, j in s_corr_list:\n            correlation_df[corr_df.index[i] +\" and \"+ corr_df.columns[j]]= v\n        correlation_df=pd.DataFrame(correlation_df,index=['Correlation'])\n    return  correlation_df.T.sort_values(by='Correlation',ascending=False)\n\n#Turn three functions on or off below:\nprintHeat() \nprintHighCorr(data,data.columns).style.set_properties(**{'background-color': 'black','color': 'white'})\n\n#calc_VIF(df[::-1]) #test for multicollinearity using VIF","metadata":{"_uuid":"f8191fa5-b571-4f3c-b139-f7a22c33404c","_cell_guid":"c772c602-12f7-4a1e-9032-66ef35a44b66","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-30T17:26:52.724303Z","iopub.execute_input":"2022-01-30T17:26:52.724635Z","iopub.status.idle":"2022-01-30T17:26:53.990309Z","shell.execute_reply.started":"2022-01-30T17:26:52.724602Z","shell.execute_reply":"2022-01-30T17:26:53.989458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Scatterplot Data\nThe most important features that were found with the highest correlation to house sales price which were the following: 'OverallQual','TotalBsmtSF','MSSubClass', 'LotArea'. This helps visually see the extent of correlations between the quantities and variables.","metadata":{"_uuid":"159136b8-e7cd-4cc7-8ba8-cae6374cfb1d","_cell_guid":"a6d87887-be0e-46e0-aa10-3b78ca6837ac","trusted":true}},{"cell_type":"code","source":"def pltFigure():\n    names=['OverallQual','TotalSF','MSSubClass', 'LotArea', ]\n    for name in names:\n        fig = plt.figure(figsize=(12,10))\n        plt.subplot(321)\n        sns.scatterplot(data=df, x=name, y=\"SalePrice\")\n        plt.show()\npltFigure()","metadata":{"_uuid":"1368f243-698a-4d6e-8f78-e48739bdfb0c","_cell_guid":"1d9d5adc-c604-47b3-8c59-5cf91277e9cf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-30T17:26:53.992074Z","iopub.execute_input":"2022-01-30T17:26:53.992695Z","iopub.status.idle":"2022-01-30T17:26:54.721051Z","shell.execute_reply.started":"2022-01-30T17:26:53.992657Z","shell.execute_reply":"2022-01-30T17:26:54.719912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression\nR-squared and accuracy produced a 0.7979 performance result:\n* The linear regression cross validated is 0.7785.\n* Both ridge and lasso regression r-squared result is 0.7785.","metadata":{"_uuid":"fd09e9f7-cd3d-4e58-a0f1-7eea4b9c23fe","_cell_guid":"015b3bb2-1fc3-40ab-a97c-b6ef88f5a040","trusted":true}},{"cell_type":"code","source":"#LINEAR REGRESSION MODEL:\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\n\n#Split the data set into training data and test data:\nx_train_data, x_test_data, y_train_data, y_test_data = train_test_split(X, y, test_size = 0.3, random_state=42)\n\nlrModel = LinearRegression()\nlrModel.fit(x_train_data, y_train_data)\n#print(model.coef_)\n#print(model.intercept_)\n\n#Generate Predictions:\npredictions = lrModel.predict(x_test_data)\n\n# plt.scatter(y_test, predictions)\nplt.hist(y_test_data - predictions)\n\n#Performance measurement:\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, r2_score\n#print(classification_report(y_test_data, predictions))\n#print(confusion_matrix(y_test_data, predictions))\n\nmetrics.mean_absolute_error(y_test_data, predictions)\nmetrics.mean_squared_error(y_test_data, predictions)\nnp.sqrt(metrics.mean_squared_error(y_test_data, predictions))\n\n#use model to predict probability that given y value is 1:\ny_pred_proba = lrModel.predict(x_test_data)\nr2 = r2_score(y_test_data, y_pred_proba).round(4) \nprint(\"Linear regression r2 score: \", r2)\n\n#CROSS VALIDATE TEST RESULTS:\nlr_score = lrModel.score(x_test_data, y_test_data).round(4)  # train test \nprint(\"Accuracy: \", lr_score)\nlr_cv = cross_validate(lrModel, X, y, cv = 5, scoring= 'r2')\nlr_cvMean=lr_cv['test_score'].mean().round(4)\nprint(lr_cvMean, \" linear regression cross validate mean\")\n\n#RIDGE REGRESSION:\nridge = Ridge(alpha = .5)  # sets alpha to a default value as baseline  \nridge.fit(x_train_data, y_train_data)\nridge_cv = cross_validate(ridge, X, y, cv = 5, scoring = 'r2')\nridge_cvMean=ridge_cv['test_score'].mean().round(4)\nprint (\"Ridge Regression R2: \", ridge_cvMean)\n\n#LASSO REGRESSION:\nlasso = Lasso(alpha = .1, normalize=True)  # sets alpha to almost zero as baseline\nlasso.fit(x_train_data, y_train_data)\nlasso_cv = cross_validate(lasso, X, y, cv = 5, scoring = 'r2')\nlasso_cvMean=lasso_cv['test_score'].mean().round(4)\nprint (\"Lasso Regression R2: \", lasso_cvMean)","metadata":{"_uuid":"3fea568e-2aef-4fe5-9ff5-5a8cb41e1f52","_cell_guid":"033cc53d-f150-48f5-a923-9d3920ad117c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-30T17:26:54.723778Z","iopub.execute_input":"2022-01-30T17:26:54.724316Z","iopub.status.idle":"2022-01-30T17:26:55.952081Z","shell.execute_reply.started":"2022-01-30T17:26:54.724268Z","shell.execute_reply":"2022-01-30T17:26:55.951320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Prediction:\nR-squared for random forest is 0.8905. The results used an n_estimator of 400 which provides the lowest mean absolute error (MAE).","metadata":{"_uuid":"73b72ce0-cbb6-4d85-930c-6b5407eab642","_cell_guid":"25ecb90c-4f97-4355-9d90-afc64eb8bea3","trusted":true}},{"cell_type":"code","source":"#RANDOM FOREST MODEL:\nfrom sklearn.ensemble import RandomForestRegressor\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n\n#TEST TO SEE WHAT n_estimators value produces the least amount of MAE error:\ndMAE={} #dictionary of n_estimators as key and MAE as value:\ndef checkMAE():\n    for n in range(100, 1401, 100):\n        forest = RandomForestRegressor(n_estimators=n, random_state = 0)\n        forest.fit(X_train, y_train)\n        y_pred = forest.predict(X_test)\n        MAE=metrics.mean_absolute_error(y_test, y_pred).round(2)\n        dMAE[n]=MAE\n        #print(\"n_estimates: \", n,  '  Mean Absolute Error:', MAE)\n\n    dMAE=sorted(((v, k) for k, v in dMAE.items()), reverse=False)\n    #print(dMAE) #[(18573.45, 400), (18632.04, 200), (18636.62, 500), (18644.81, 300), (18651.96, 600),\n\nnum=400 #lowest n_estimators in MAE in sorted dic\nforest = RandomForestRegressor(n_estimators=num, random_state = 0)\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)\nrf_r2 = r2_score(y_test, y_pred).round(4) \nprint(\"Random forest r2: \", rf_r2) #Random forest r2:  0.8905","metadata":{"_uuid":"5b2bf4b9-b23c-4eb2-8fdc-7a783878d836","_cell_guid":"98395503-e38d-4c1b-95ad-6252899a1025","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-30T17:26:55.953337Z","iopub.execute_input":"2022-01-30T17:26:55.954115Z","iopub.status.idle":"2022-01-30T17:27:01.176631Z","shell.execute_reply.started":"2022-01-30T17:26:55.954068Z","shell.execute_reply":"2022-01-30T17:27:01.175553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree\nThe decision tree r-sqaured is 0.6948 which is much lower than the other predictive models. More tests will need to be run to verify if the data has been underfit.","metadata":{"_uuid":"57ce9d98-d5ea-4b62-9a31-480d72d838ec","_cell_guid":"a64233cb-544c-499d-b245-29c3b64528ab","trusted":true}},{"cell_type":"code","source":"#DECISION TREE\nfrom sklearn.tree import DecisionTreeRegressor\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n\n#FIND best_tree_size LEAF NODES:\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=42)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\nmaeDic={} #dictionary  key=leaf  mae=value\nfor leaf in candidate_max_leaf_nodes:\n    mae=get_mae(leaf, X_train, X_test, y_train, y_test)\n    maeDic[leaf]=mae\n\nbest_tree_size = sorted(maeDic, key=lambda x : maeDic[x])[0]\n\n#MAKE PREDICTION:\ntree = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=42)\ntree.fit(X, y)\ny_pred = tree.predict(X_test)\n\ntree_r2 = r2_score(y_test, y_pred).round(4)\nprint(\"Decision tree r2: \", tree_r2)\n\ndef printReports(y_test, y_pred):\n    print(classification_report(y_test, y_pred))\n    print(confusion_matrix(y_test, y_pred))","metadata":{"_uuid":"c2cc7361-9968-4ca2-b283-3f85ec7e5de5","_cell_guid":"abf531b1-1acd-4f5e-baad-3776a13d174c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-30T17:27:01.177890Z","iopub.execute_input":"2022-01-30T17:27:01.178121Z","iopub.status.idle":"2022-01-30T17:27:01.306003Z","shell.execute_reply.started":"2022-01-30T17:27:01.178093Z","shell.execute_reply":"2022-01-30T17:27:01.305164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient Boost regression and Feature Importance\n\nGradient boosting regression r-squared is 0.5721, and the mean squared error (MSE) on test set: 0.1063.","metadata":{"_uuid":"5c2f0d16-dc0e-4e1c-a459-7f2c5dd2e392","_cell_guid":"12982f71-20b5-46b6-86bf-9e63abdad632","trusted":true}},{"cell_type":"code","source":"#GRADIENT BOOST REGRESSION:\nfrom sklearn import datasets, ensemble\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=13)\n\nparams = {\n \"n_estimators\": 500, \"max_depth\": 4, \"min_samples_split\": 5, \"learning_rate\": 0.01,\n}\n\nreg = ensemble.GradientBoostingRegressor(**params)\nreg.fit(X_train, y_train)\n\ny_pred = reg.predict(X_test)\ngbr_r2 = r2_score(y_test, y_pred).round(4) \nprint(\"Gradient boosting regression r2: \", gbr_r2) \n\nmse = mean_squared_error(y_test, reg.predict(X_test))\nprint(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n\n#FEATURE IMPORTANCE:\ncols=X.columns\nfeature_importance = reg.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + 0.5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align=\"center\")\nplt.yticks(pos, np.array(cols)[sorted_idx])\nplt.title(\"Feature Importance (MDI)\")\n\nresult = permutation_importance(\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\nplt.subplot(1, 2, 2)\nplt.boxplot(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=np.array(cols)[sorted_idx],\n)\nplt.title(\"Permutation Importance (test set)\")\nfig.tight_layout()\nplt.show()","metadata":{"_uuid":"9adafbb8-a392-40a5-b9d8-5020d99a5596","_cell_guid":"05501529-eed2-42c8-9431-af61f4887129","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-31T01:22:26.955649Z","iopub.execute_input":"2022-01-31T01:22:26.955975Z","iopub.status.idle":"2022-01-31T01:22:31.139706Z","shell.execute_reply.started":"2022-01-31T01:22:26.955939Z","shell.execute_reply":"2022-01-31T01:22:31.139001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final House Price Pediction","metadata":{"_uuid":"769e64a6-03bd-41cc-8ef0-36641509dd51","_cell_guid":"79dffa57-1d3c-4ba9-8869-414566743eaa","trusted":true}},{"cell_type":"code","source":"#Predict the house price for all 1459 houses:\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=13)\n\nforest = RandomForestRegressor(n_estimators=500, random_state = 0)\nforest.fit(X_train, y_train)\npreds = forest.predict(X_test)\n\nsale_price_scaler = preprocessing.MinMaxScaler()\ntargets = df['SalePrice'].values.reshape(-1, 1)\ntarget_scaled = sale_price_scaler.fit_transform(targets)\n\nids = test['Id']#.values  # 1459 count\n\npreds = sale_price_scaler.inverse_transform(preds.reshape(-1, 1)).squeeze(1)\npreds = np.nan_to_num(preds)\n\nout_df = pd.DataFrame({\"Id\": ids, \"SalePrice\": preds})\nout_df.head() #array length 146 does not match index length 1459","metadata":{"_uuid":"2c282ede-57a6-496f-8ab2-63760ede77da","_cell_guid":"e694ea16-417e-468c-b76a-9575802ae8cc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-31T01:21:40.497535Z","iopub.execute_input":"2022-01-31T01:21:40.497851Z","iopub.status.idle":"2022-01-31T01:21:48.753785Z","shell.execute_reply.started":"2022-01-31T01:21:40.497814Z","shell.execute_reply":"2022-01-31T01:21:48.752682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/ryanholbrook/feature-engineering-for-house-prices\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.99, random_state=13)\n#y_train = df.loc[:,\"SalePrice\"]\n\n# So, we need to log-transform y to train and exp-transform the predictions:\nreg.fit(X_train, np.log(y_train))\npredictions = np.exp(reg.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions}).sort_values(by='Id')\nprint(output.shape) #(438, 2)\nprint(output.head())\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n#ValueError: Found input variables with inconsistent numbers of samples: [1022, 1460]","metadata":{"_uuid":"2dd3478a-3a64-4af2-8b77-c92b32683686","_cell_guid":"82d5c9d1-09a2-46d2-9bf4-ac74fa8777b7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-31T02:12:35.144455Z","iopub.execute_input":"2022-01-31T02:12:35.144762Z","iopub.status.idle":"2022-01-31T02:12:35.347436Z","shell.execute_reply.started":"2022-01-31T02:12:35.144725Z","shell.execute_reply":"2022-01-31T02:12:35.346560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Helpful Resources:\n* https://www.kaggle.com/niekvanderzwaag/housing-price-prediction-regression\n* https://www.kaggle.com/ryanholbrook/feature-engineering-for-house-prices\n* https://www.kaggle.com/gprakhar579/house-price-prediction-ml#Outlier-detection-in-training-data-set\n* https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/","metadata":{"_uuid":"f4deb84a-58c3-45a6-8ef8-121b16248f61","_cell_guid":"71a5c9dc-61b6-4861-88f7-c9de315348e2","trusted":true}}]}