{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Credit Card Fraud Dectection\n\nThis will use machine learning and predictive modeling to help detect fradulent credit card transaction.\nThe original data csv file can be found here: https://www.kaggle.com/mlg-ulb/creditcardfraud\nThe columns with the first five rows can be seen below in the next section.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom statistics import mean\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\nimport warnings\nwarnings.filterwarnings('ignore')\n\nkaggleFile=r'/kaggle/input/creditcardfraud/creditcard.csv'\ndata=pd.read_csv(kaggleFile)\nprint(data.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-11T02:42:23.836985Z","iopub.execute_input":"2022-02-11T02:42:23.837712Z","iopub.status.idle":"2022-02-11T02:42:26.616593Z","shell.execute_reply.started":"2022-02-11T02:42:23.837667Z","shell.execute_reply":"2022-02-11T02:42:26.615639Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Find missing data","metadata":{}},{"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf = data.select_dtypes(include=numerics)\n\nprint(\"Missing data by column:\")\nfindNA=df.isnull().sum().sort_values(ascending=False)/len(data)\nprint(findNA.head()) #There are no missing values","metadata":{"execution":{"iopub.status.busy":"2022-02-11T02:42:26.618492Z","iopub.execute_input":"2022-02-11T02:42:26.618942Z","iopub.status.idle":"2022-02-11T02:42:26.644924Z","shell.execute_reply.started":"2022-02-11T02:42:26.618895Z","shell.execute_reply":"2022-02-11T02:42:26.644112Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Correlations using heatmap\nNo multicollinearity was detected. ","metadata":{}},{"cell_type":"code","source":"def printHeat():\n    corr = df.corr()\n    #print(corr)\n    highly_corr_features = corr.index[abs(corr[\"Class\"])>0.2]\n    plt.figure(figsize=(10,10))\n    heat = sns.heatmap(data[highly_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n    top10=corr[\"Class\"].sort_values(ascending=False).head(10)\n    print(heat)\n    #print(top10) # top ten correlations\n\n#print correlation between features\ndef printHighCorr(df, features, threshold=0.7):\n    print(\"Highly correlated variables above: \", threshold)\n    corr_df = df[features].corr() # get correlations\n    correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n    correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n    s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n    correlation_df={}\n    if s_corr_list == []:\n        print(\"There are no highly correlated features with correlation above\", threshold)\n    else:\n        for v, i, j in s_corr_list:\n            correlation_df[corr_df.index[i] +\" and \"+ corr_df.columns[j]]= v\n        correlation_df=pd.DataFrame(correlation_df,index=['Correlation'])\n    return  correlation_df.T.sort_values(by='Correlation',ascending=False)\n\n#Turn functions on or off below:\nprintHeat() \n#printHighCorr(data,data.columns).style.set_properties(**{'background-color': 'black','color': 'white'})","metadata":{"execution":{"iopub.status.busy":"2022-02-11T03:26:43.529539Z","iopub.execute_input":"2022-02-11T03:26:43.530212Z","iopub.status.idle":"2022-02-11T03:26:44.823349Z","shell.execute_reply.started":"2022-02-11T03:26:43.530170Z","shell.execute_reply":"2022-02-11T03:26:44.822491Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### Split Data\nUse 70% of the train data to predict the accuracy of the remaining 30% of the test data.","metadata":{}},{"cell_type":"code","source":"X=df.drop('Class', axis=1)\ny=df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T02:42:26.697183Z","iopub.execute_input":"2022-02-11T02:42:26.697523Z","iopub.status.idle":"2022-02-11T02:42:26.822827Z","shell.execute_reply.started":"2022-02-11T02:42:26.697490Z","shell.execute_reply":"2022-02-11T02:42:26.821919Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression Predictive Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogReg = LogisticRegression()\n\n#Train the model and create predictions\nlogReg.fit(X_train, y_train)\npredictions = logReg.predict(X_test)\n\n#use model to predict probability that given y value is 1:\ny_pred_proba = logReg.predict_proba(X_test)[::,1]\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, y_pred_proba), 4 ) \nprint(\"AUC is: \", auc)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T03:55:42.966463Z","iopub.execute_input":"2022-02-11T03:55:42.966999Z","iopub.status.idle":"2022-02-11T03:55:45.680279Z","shell.execute_reply.started":"2022-02-11T03:55:42.966957Z","shell.execute_reply":"2022-02-11T03:55:45.679504Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"#### Resources\n1. https://www.kaggle.com/jdelamorena/recall-97-by-using-undersampling-neural-network","metadata":{}}]}