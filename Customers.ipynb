{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customer Satisfaction\n### Business Problem Introduction\nFrom frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving. Santander Bank is asking Kagglers to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late.\nIn this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience. The original dataset can be found on Kaggle: https://www.kaggle.com/c/santander-customer-satisfaction\n\n### Quick Summary\nSince the data is unbalanced, r_squared is not the best metric to be used. Therefore, we will use AUC (Area Under the Curve) as a metric:\n* Decision tree is:  0.841\n* Linear regression is:  0.7838\n* Random forest AUC:  0.6872\n* Logistic regression is:  0.5782\n\n--------------\n## View Train Data\nThe train data is the most important dataset in which we try to find trends on the TARGET column, Y indepdendent variable.","metadata":{"execution":{"iopub.status.busy":"2022-02-26T01:02:28.405424Z","iopub.execute_input":"2022-02-26T01:02:28.405674Z","iopub.status.idle":"2022-02-26T01:02:28.411147Z","shell.execute_reply.started":"2022-02-26T01:02:28.405647Z","shell.execute_reply":"2022-02-26T01:02:28.410434Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom statistics import mean\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\nimport sklearn.metrics as metrics\n\nsample=r'/kaggle/input/santander-customer-satisfaction/sample_submission.csv'\ntrain=r'/kaggle/input/santander-customer-satisfaction/train.csv'\ntest=r'/kaggle/input/santander-customer-satisfaction/test.csv'\n#sample=r'C:\\Users\\sschm\\Desktop\\kaggle\\customer\\sample_submission.csv'\n#train=r'C:\\Users\\sschm\\Desktop\\kaggle\\customer\\train.csv'\n#test=r'C:\\Users\\sschm\\Desktop\\kaggle\\customer\\test.csv'\n\ndata=pd.read_csv(train)\ndata.drop_duplicates() #no duplicates\nprint(data.shape) #(76020, 371)\ndata.head() ","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:18:38.027009Z","iopub.execute_input":"2022-03-23T22:18:38.027801Z","iopub.status.idle":"2022-03-23T22:18:44.024602Z","shell.execute_reply.started":"2022-03-23T22:18:38.027651Z","shell.execute_reply":"2022-03-23T22:18:44.023573Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Numeric DataFrame","metadata":{}},{"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf = data.select_dtypes(include=numerics)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:18:44.026719Z","iopub.execute_input":"2022-03-23T22:18:44.027852Z","iopub.status.idle":"2022-03-23T22:18:44.188145Z","shell.execute_reply.started":"2022-03-23T22:18:44.027791Z","shell.execute_reply":"2022-03-23T22:18:44.187038Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Find Missing Values\nThe dataset is still structured as (76020, 371) meaning all the columns must have been numeric. There are no missing values. ","metadata":{}},{"cell_type":"code","source":"#search for columns with missing values:\ndef findNA():\n    print(\"Missing data by column as a percent:\")\n    findNA=df.isnull().sum().sort_values(ascending=False)/len(df)\n    print(findNA.head())\nfindNA() ","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:18:44.189635Z","iopub.execute_input":"2022-03-23T22:18:44.189906Z","iopub.status.idle":"2022-03-23T22:18:44.243145Z","shell.execute_reply.started":"2022-03-23T22:18:44.189860Z","shell.execute_reply":"2022-03-23T22:18:44.242244Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Examine Target Column\nCustomer satisfaction is in 1 and 0. Almost all of the TARGET column is 0 which is at 0.9604%.  Also, since the target values are only one and zero we can use logistic regression to further analyze any trends in the data. ","metadata":{}},{"cell_type":"code","source":"target=df['TARGET'].unique()\nprint(target)\n\n#Check for unbalanced data:\ntargets=list(df['TARGET'])\nzero = targets.count(0)\none = targets.count(1)\ntotal=round(zero/len(targets),4)\nprint(total)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:18:44.245010Z","iopub.execute_input":"2022-03-23T22:18:44.245279Z","iopub.status.idle":"2022-03-23T22:18:44.264492Z","shell.execute_reply.started":"2022-03-23T22:18:44.245249Z","shell.execute_reply":"2022-03-23T22:18:44.263832Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap","metadata":{}},{"cell_type":"code","source":"def printHeat():\n    corr = df.corr()\n    #print(corr)\n    y='TARGET'\n    highly_corr_features = corr.index[abs(corr[y])>0.05]\n    plt.figure(figsize=(10,10))\n    heat = sns.heatmap(df[highly_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n    top10=corr[y].sort_values(ascending=False).head(10)\n    print(heat)\n    print(\"Top 10 Correlations:\\n\", top10) # top ten correlations\nprintHeat()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:18:44.265546Z","iopub.execute_input":"2022-03-23T22:18:44.265845Z","iopub.status.idle":"2022-03-23T22:19:13.647855Z","shell.execute_reply.started":"2022-03-23T22:18:44.265814Z","shell.execute_reply":"2022-03-23T22:19:13.647081Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Identify Highly Correlated Features:\nFirst, we must remove highly correlated features that are above .80 correlation. Normally, creating a heapmap visualization is helpful but with 371 column features, will not be memory efficient unless using a supercomputer.","metadata":{}},{"cell_type":"code","source":"print(df.shape, \" before removing highly correlated variables.\")\n# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n\ndf=df.drop(to_drop, axis = 1)\nprint(df.shape, \" after removing highly correlated variables.\")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:13.649139Z","iopub.execute_input":"2022-03-23T22:19:13.649995Z","iopub.status.idle":"2022-03-23T22:19:42.035501Z","shell.execute_reply.started":"2022-03-23T22:19:13.649958Z","shell.execute_reply":"2022-03-23T22:19:42.034510Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Split the Data\nUse 70% of the train data to predict the accuracy of the remaining 30% of the test data.","metadata":{"execution":{"iopub.status.busy":"2022-02-28T00:53:12.026269Z","iopub.execute_input":"2022-02-28T00:53:12.026705Z","iopub.status.idle":"2022-02-28T00:53:12.030847Z","shell.execute_reply.started":"2022-02-28T00:53:12.026675Z","shell.execute_reply":"2022-02-28T00:53:12.03006Z"}}},{"cell_type":"code","source":"X=df.drop('TARGET', axis=1)\ny=df['TARGET']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:42.037088Z","iopub.execute_input":"2022-03-23T22:19:42.037462Z","iopub.status.idle":"2022-03-23T22:19:42.234872Z","shell.execute_reply.started":"2022-03-23T22:19:42.037428Z","shell.execute_reply":"2022-03-23T22:19:42.233862Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\nAUC for logistic regression is:  0.5782.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogReg = LogisticRegression(solver='liblinear') #solver param gets rid of encoder error\n\n#Train the model and create predictions\nlogReg.fit(X_train, y_train)\nlogPredict = logReg.predict_proba(X_test)[::,1]\n\n#calculate AUC of model\nauc = round( metrics.roc_auc_score(y_test, logPredict), 4 ) \nprint(\"AUC for logistic regression is: \", auc)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:42.236594Z","iopub.execute_input":"2022-03-23T22:19:42.236851Z","iopub.status.idle":"2022-03-23T22:19:45.351966Z","shell.execute_reply.started":"2022-03-23T22:19:42.236814Z","shell.execute_reply":"2022-03-23T22:19:45.350589Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression\nAUC for linear regression is:  0.7838","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n\n#Fit and predict:\nlrModel = LinearRegression()\nlrModel.fit(X_train, y_train)\nlrPredict = lrModel.predict(X_test)\n\n# plt.scatter(y_test, predictions)\nplt.hist(y_test - lrPredict)\n\n#Linear Metrics:\nauc = round( metrics.roc_auc_score(y_test, lrPredict), 4 ) \nr2 = r2_score(y_test, lrPredict).round(4) \nprint(\"AUC for linear regression is: \", auc)\nprint(\"Linear regression r2 score: \", r2)\n\n#CROSS VALIDATE TEST RESULTS:\nlr_score = lrModel.score(X_test, y_test).round(4)  # train test \nprint(\"Linear Accuracy: \", lr_score)\nlr_cv = cross_validate(lrModel, X, y, cv = 5, scoring= 'r2')\nlr_cvMean=lr_cv['test_score'].mean().round(4)\nprint(lr_cvMean, \" linear regression cross validate mean\")\n\ndef linearReports():\n    print(model.coef_)    \n    print(model.intercept_)\n    print(classification_report(y_test_data, lrPredict))\n    print(confusion_matrix(y_test_data, lrPredict))\n    metrics.mean_absolute_error(y_test, lrPredict)\n    np.sqrt(metrics.mean_squared_error(y_test, lrPredict))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:45.354102Z","iopub.execute_input":"2022-03-23T22:19:45.355948Z","iopub.status.idle":"2022-03-23T22:19:51.701002Z","shell.execute_reply.started":"2022-03-23T22:19:45.355889Z","shell.execute_reply":"2022-03-23T22:19:51.698873Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree\nThe best max_leaf_node is 250.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n#FIND best_tree_size LEAF NODES:\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=42)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\ndef calcLeaf():\n    candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n    maeDic={} #dictionary  key=leaf  mae=value\n    for leaf in candidate_max_leaf_nodes:\n        mae=get_mae(leaf, X_train, X_test, y_train, y_test)\n        maeDic[leaf]=mae\n\n    best_tree_size = sorted(maeDic, key=lambda x : maeDic[x])[0]\n    print(best_tree_size, \" best_tree_size\")\n\nbest_tree_size=250\n    \n#MAKE PREDICTION:\ntree = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=42)\ntree.fit(X, y)\ny_pred = tree.predict(X_test)\n\n#AUC and r2 metric:\ntreeR2 = r2_score(y_test, y_pred).round(4)\ntreeAUC = round( metrics.roc_auc_score(y_test, y_pred), 4 ) \nprint(\"AUC for decision tree is: \", treeAUC)\n\ndef printReports(y_test, y_pred):\n    print(classification_report(y_test, y_pred))\n    print(confusion_matrix(y_test, y_pred))\n    \n    #Mean Sqaured Error:\n    treeMSE=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n    print(round(treeMSE, 4), \" is tree MSE \")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:51.702480Z","iopub.status.idle":"2022-03-23T22:19:51.703844Z","shell.execute_reply.started":"2022-03-23T22:19:51.703491Z","shell.execute_reply":"2022-03-23T22:19:51.703531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n#Check for Error and find Best n_estimators:\ndef checkMAE():\n    print(\"Starting MAE:\")\n    dMAE={} #dictionary of n_estimators as key and MAE as value:\n    for n in range(2, 500, 100):\n        forest = RandomForestRegressor(n_estimators=n, random_state = 0)\n        forest.fit(X_train, y_train)\n        y_pred = forest.predict(X_test)\n        MAE=metrics.mean_absolute_error(y_test, y_pred).round(2)\n        dMAE[n]=MAE\n        print(\"n_estimates: \", n,  '  Mean Absolute Error:', MAE)\n\n    dMAE=sorted(((v, k) for k, v in dMAE.items()), reverse=False)\n    print(dMAE)\n#checkMAE() #turn function on or off by uncommenting\n\ndef forest():\n    num=10\n    forest = RandomForestRegressor(n_estimators=num, random_state = 0)\n    forest.fit(X_train, y_train)\n    y_pred = forest.predict(X_test)\n\n    #Print Metrics:\n    forest_r2 = r2_score(y_test, y_pred).round(4)  \n    forest_auc = round( metrics.roc_auc_score(y_test, y_pred), 4 ) \n    print(\"Random forest AUC: \", forest_auc) \n    print(\"Random forest r2: \", forest_r2)\n\ndef forestReports():\n    mae=metrics.mean_absolute_error(y_test, y_pred).round(2)\n    print(\"Random forest MAE: \", mae)\n    \nforest()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:51.705572Z","iopub.status.idle":"2022-03-23T22:19:51.706238Z","shell.execute_reply.started":"2022-03-23T22:19:51.705941Z","shell.execute_reply":"2022-03-23T22:19:51.705970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Dataset\nHere we will make final predictions for the test dataset.","metadata":{}},{"cell_type":"code","source":"testDF=pd.read_csv(test)\ntestDF.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:51.709184Z","iopub.status.idle":"2022-03-23T22:19:51.709835Z","shell.execute_reply.started":"2022-03-23T22:19:51.709570Z","shell.execute_reply":"2022-03-23T22:19:51.709598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineer Test Dataset","metadata":{}},{"cell_type":"code","source":"features=list(X.columns)\ntestDF=testDF[features]\ntestDF=testDF.fillna(testDF.mean())\ntestDF.head() #5 rows × 105 columns","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:51.711194Z","iopub.status.idle":"2022-03-23T22:19:51.711829Z","shell.execute_reply.started":"2022-03-23T22:19:51.711557Z","shell.execute_reply":"2022-03-23T22:19:51.711591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Final Predictions","metadata":{}},{"cell_type":"code","source":"test_predictions = tree.predict(testDF).round(1)\ntest_predictions=np.where(test_predictions<0, 0, test_predictions)\nID=testDF['ID']\ntupleData = list(zip(ID, test_predictions))\noutput = pd.DataFrame(tupleData, columns = ['ID', 'TARGET'])\nprint(output.shape)\noutput.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:51.713227Z","iopub.status.idle":"2022-03-23T22:19:51.713859Z","shell.execute_reply.started":"2022-03-23T22:19:51.713596Z","shell.execute_reply":"2022-03-23T22:19:51.713622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My Final Summission","metadata":{}},{"cell_type":"code","source":"output.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T22:19:51.715188Z","iopub.status.idle":"2022-03-23T22:19:51.715818Z","shell.execute_reply.started":"2022-03-23T22:19:51.715556Z","shell.execute_reply":"2022-03-23T22:19:51.715583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\n1. https://www.kaggle.com/rahulanand0070/feature-selection\n2. https://www.kaggle.com/solegalli/feature-selection-with-feature-engine","metadata":{}}]}